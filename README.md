# ğŸ“Š IngestÃ£o de Dados em Batch com Airflow e Delta Lake

O objetivo deste projeto Ã© **simular um pipeline de ingestÃ£o de dados transacionais em batch**, aplicando a arquitetura em camadas (medalhÃ£o): **source â†’ bronze â†’ silver**.  

Com isso, busca-se:  
- âœ… **Gerar dados mockados** de um sistema transacional para simular o fluxo real;  
- âœ… **Orquestrar a ingestÃ£o diÃ¡ria** desses arquivos utilizando o **Apache Airflow**;  
- âœ… **Processar os dados** em larga escala com **Apache Spark**;  
- âœ… **Armazenar em formato Delta Lake**, garantindo versionamento, consistÃªncia e eficiÃªncia;  
- âœ… **Fornecer um exemplo didÃ¡tico** de como funcionaria um fluxo de ingestÃ£o de dados em um ambiente de **engenharia de dados moderna**.  

Este projeto tem carÃ¡ter **educacional e prÃ¡tico**, servindo como base para estudos em **engenharia de dados, processamento distribuÃ­do e orquestraÃ§Ã£o de pipelines**. 

Todo o pipeline Ã© executado **localmente**, orquestrado por uma instÃ¢ncia do **Apache Airflow**, e utilizando **Apache Spark** com suporte ao **Delta Lake**.

âš ï¸ Obs: Foi utilizado **PySpark** por questÃµes didÃ¡ticas. Em um contexto real, a massa de dados precisaria ser maior para justificar essa tecnologia mas, para integraÃ§Ã£o com o Delta o PySpark Ã© a escolha mais prÃ¡tica.

---

## ğŸ—‚ Estrutura do Projeto

```
project-root/
â”‚
â”œâ”€â”€ mock_data/
â”‚   â””â”€â”€ MOCK_DATA.csv              # Base de dados completa usada como origem
â”‚
â”œâ”€â”€ datalake/                      # Pasta externa contendo os dados organizados por camadas. VocÃª precisa criar essa pasta e apontar na variÃ¡vel "DATALAKE_PATH" dentro dos arquivos
â”‚   â”œâ”€â”€ source/
â”‚   â”‚   â””â”€â”€ transaction_system/    # Arquivos CSV diÃ¡rios de entrada
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â””â”€â”€ transaction_data/      # Dados no formato Delta Lake (raw zone)
â”‚   â””â”€â”€ silver/
â”‚       â””â”€â”€ transaction_data/      # Dados tratados e deduplicados no formato Delta Lake
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ ingest_transaction_data.py # DAG do Airflow responsÃ¡vel pela ingestÃ£o diÃ¡ria. VocÃª pode utilizar o comando 'cp' para copiar essa DAG para sua pasta de DAGs do Airflow.
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ mock_data_spliter.py       # Divide o MOCK_DATA.csv em 10 partes com datas diferentes
â”‚   â”œâ”€â”€ ingest_source_to_bronze.py # Realiza ingestÃ£o dos arquivos CSV para camada bronze
â”‚   â””â”€â”€ ingest_bronze_to_silver.py # Atualiza a camada silver com dados incrementais da bronze
â”‚
â”œâ”€â”€ requirements.txt               # Bibliotecas necessÃ¡rias para execuÃ§Ã£o
â””â”€â”€ README.md                      # Este arquivo
```

âš ï¸ **Importante:**  
Crie manualmente a pasta `datalake/` na raiz do projeto antes da execuÃ§Ã£o e ajuste o caminho absoluto na variÃ¡vel `DATALAKE_PATH` dentro dos scripts.

---

## âš™ï¸ Como Executar

### 1. Instale as dependÃªncias
```bash
pip install -r requirements.txt
```

### 2. Gere os arquivos simulados
```bash
python scripts/mock_data_spliter.py
```

### 3. Inicie o Airflow standalone
```bash
airflow standalone
```

### 4. Ative a DAG no painel do Airflow
Acesse o Airflow em [http://localhost:8080](http://localhost:8080) e ative a DAG `INGEST_TRANSACTION_DATA`.

> A DAG estÃ¡ agendada para rodar todos os dias Ã s 03:00 (`0 3 * * *`), com suporte a **catchup** de dias anteriores. A simulaÃ§Ã£o inclui falhas nos dias em que nÃ£o existem arquivos.

---


## ğŸ“¦ Tecnologias Utilizadas

- **Apache Spark** (Standalone)
- **Delta Lake**
- **Apache Airflow** (Standalone)
- **Python 3.10+**
- **Pandas**
- **Linux**

---

## ğŸ“„ LicenÃ§a
Este projeto Ã© livre para reproduÃ§Ã£o e customizaÃ§Ã£o, sob licenÃ§a MIT.

## ğŸ“¬ Contato

- [LinkedIn](https://www.linkedin.com/in/thiago-cbferreira/)  
- [GitHub](https://github.com/ThiagoCBF) 
